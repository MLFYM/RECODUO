# :inbox_tray:데이터 수집

> 얼굴 감정 인식을 기반으로 하는 음악 추천 서비스, 들려듀오는 크게 **감정 분류 모델**과 **음악 추천 알고리즘** 두 부분으로 나눌 수 있다. 사용자의 감정을 제대로 판별하기 위해서는 모델의 정확도를 확보하는 것이 중요하며, 그를 결정하는 주요한 요소는 훈련 데이터의 질과 양이다. 
>
> 모델 훈련을 위해 수집 및 검증한 데이터는 **총 3,247만 장**으로, 수집 과정은 이하와 같다.



## 1. 검증 데이터 수집

### 1.1 Ai hub K-FACE Dataset

![다운로드](https://user-images.githubusercontent.com/58945760/87050140-d5413900-c238-11ea-8dd0-8bd24692f648.jpg)

![캡처](https://user-images.githubusercontent.com/58945760/87049625-30bef700-c238-11ea-97df-1ed8b98f66f7.PNG)

> 전체 데이터의 70% 이상을 차지한 Main Dataset

- [Ai hub 사이트](http://www.aihub.or.kr/aidata/73)에서 구체적인 데이터 정보를 확인할 수 있으며 [데이터 신청](http://kface.kist.re.kr/#/) 과정을 거쳐 저화질, 중화질, 고화질 데이터를 다운로드할 수 있다.  

- 데이터 신청을 위해서는 K-FACE 공식 홈페이지에 **필수적으로** 가입하여야 한다. 또한 **데이터 활용 계획서**와 **개인정보 동의서**를 별도 제출해야 하며 동의서 내 **소속 기관장의 서명**이 요구된다.  신청 후 승인이 날 때까지는 3~4일 정도가 소요된다. 

- 학술적인 목적에 한해 사용할 수 있으며, 상업적인 용도로는 이용할 수 없다.

- 표정으로 감정을 분류하는 모델을 만드는 만큼 섬세한 얼굴 변화까지 잡아낼 수 있도록 **고화질** 데이터를 신청하여 다운로드하였다. 

- 총 데이터 수는 **3,240만 장**이다. 

  

### 1.2 FFHQ Dataset

![보조데이터](https://user-images.githubusercontent.com/58945760/87051097-17b74580-c23a-11ea-8d07-0633189daced.PNG)

> 보다 정확한 모델 평가와 `overfitting`을 방지하기 위한 Sub Dataset

- [해당 Github](https://github.com/NVlabs/ffhq-dataset)에서 데이터 정보 및 라이센스 관련 사항을 확인할 수 있으며, 데이터는 [Google drive](https://drive.google.com/drive/folders/1tZUcXDBeOibC6jcMCtgRRz67pzrAHeHL?usp=drive_open)에서 다운로드할 수 있다. 

![캡처](https://user-images.githubusercontent.com/58945760/87054406-0708ce80-c23e-11ea-9cd5-1c89c28fc96a.PNG)

- 다운로드 가능한 파일은 위와 같으며, 특별한 신청 없이 누구나 이용 가능하다. 
- 감정 분류의 정확도를 높이기 위해 얼굴을 따로 `crop`한 1024x1024 이미지를 다운로드하였다. 
- 총 이미지는 **7만 장**이다. 



## 2. 데이터 선별  

> 데이터를 충분히 수집했다면 그 다음 과정은 모델을 훈련하기 위한 `dataset` 구성이다. 이른바 **데이터 전처리**의 시작이다. 데이터 전처리를 하는 이유는 후에 설명하겠지만, 기본적으로는 모델 학습의 효율을 높이기 위해서다. 



> 데이터는 질보다 양이라고들 하지만, 아무리 많은 데이터가 있어봤자 풀고 싶은, 해결하고 싶은 문제와 전혀 상관없는 데이터라면 모델 훈련에 써봤자 아무 소용이 없다. 
>
> 간단한 예를 들어보자. **어떤 사람의 얼굴 사진**을 보고 **남녀를 구분할 수 있는 모델**을 만들려고 했다 치자. 그런데 모델을 훈련하는 데 쓰는 데이터가 **개, 고양이 같은 동물 사진**이라면 어떻게 될까? 사람 사진은 사람 사진인데 손이나 발만 찍은 사진이라면? 뒷모습만 찍은 사진들이라면?  당연하지만 그런 사진으로 모델 훈련을 해봤자 모델이 남녀를 구분할 턱이 없다. **그 모델을 훈련한 시간이 아깝고, 그 연산을 하느라 쓰인 GPU가 아까울 뿐이다**.  
>
> 더 쉬운 예시를 들어보자. 당장 내일이 수학 쪽지시험이라 공부를 해야 한다. 쪽지시험 범위는 미적분인데 정작 전날밤에 죽어라 푼 문제는 집합 문제다. 시험을 잘 볼 수 있을 턱이 없다.  **데이터 전처리란, 다음날 볼 쪽지시험 100점을 맞을 수 있는 기똥찬 연습문제를 만드는 과정이다**.   



그럼 어떻게 데이터를 골라내야 할까? 우선 Main dataset인 K-FACE dataset의 아키텍처를 확인해보자.

![](https://github.com/k-face/k-face_2019/raw/master/image/Amount_of_the_data.png)



먼저 확인해야 할 것은 우리가 어떤 문제를 풀기 위해 모델을 만들고 있느냐다. 우리가 풀고 싶은 문제는 얼굴 사진만 보고 그 사진 속의 인물이 어떤 감정인지 판단하는 것이다. 보다 정확히 말하면, 웹캠으로 촬영한 얼굴 사진을  받아들여(`input`)하여 감정을 분류하여 출력(`output`)하는 모델을 만들고 싶은 것이다. 

데이터 선별 과정은 다음과 같다.

![이미지 선별 과정](https://user-images.githubusercontent.com/58945760/87242238-cbe8e400-c465-11ea-9bea-586dc194e5a6.PNG)

 선별 과정에서 가장 중점을 둔 것은 데이터를 선택하는 기준이 아닌 **제외**하는 기준을 설정한 것이다. 모델의 `input`이 될 데이터는 웹캠으로 촬영한 얼굴의 이미지다. 일반적으로 노트북에 달려 있는 웹캠의 위치를 고려할 때, `input` 데이터는 정면에 가까운 얼굴 사진이 될 가능성이 높다. 

또한 감정을 분류하기 위해서는 데이터 확인 시 표정이 명확히 보여야만 한다. 표정에서 감정을 읽어내어야 하므로 잘 보이면 보일수록 좋다. 당연히 육안으로 확인할 수 있어야 하며, 이러한 확인을 방해하는 액세서리가 있어서는 안 된다.  따라서 데이터 제외 기준은 다음과 같다.



- 얼굴 전체가 보이지 않는 각도의 사진

- 육안으로 표정 식별이 불가능한 사진

  

위 기준에 따라 필요없는 데이터를 제외하였고, **다양한 모델링**을 시도하기 위해 총 데이터 수의 상한선을 **최대 45,000 장으로 조정**하였다. 
